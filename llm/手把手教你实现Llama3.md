**è®©æˆ‘ä»¬ä»å¤´å¼€å§‹ï¼Œæ‰‹æŠŠæ‰‹æ•™ä½ ï¼Œä¸€æ­¥æ­¥å®ç°Llama 3ï¼**

> å‚è€ƒ&æ¬è¿ï¼šhttps://github.com/naklecha/llama3-from-scratch    
>
> åŸæ–‡åªç»™äº†æ¨¡å‹ä»£ç ï¼Œå‡ ä¹æ²¡ä»€ä¹ˆè§£é‡Šï¼Œå¹¶æœ‰éƒ¨åˆ†é”™è¯¯ï¼Œè¿™é‡Œåšäº†è¯¦ç»†è¡¥å……ï¼
>
> é˜…è¯»æœ¬æ–‡ä¹‹å‰ï¼Œå»ºè®®é˜…è¯»ä¸€äº›Transformerçš„åŸºç¡€çŸ¥è¯†ï¼šhttps://jalammar.github.io/illustrated-transformer  
------

æœ¬æ–‡é‡‡ç”¨metaæä¾›çš„Llama 3æ¨¡å‹æ–‡ä»¶ï¼ˆéHFæ ¼å¼ï¼‰ï¼Œä»¥ä¸‹æ˜¯å®˜æ–¹ä¸‹è½½é“¾æ¥ï¼Œé€‰æ‹©å…¶ä¸€å³å¯ï¼š

ï¼ˆ1ï¼‰https://llama.meta.com/llama-downloads    

ï¼ˆ2ï¼‰https://huggingface.co/meta-llama/Meta-Llama-3-8B  

æ‰€éœ€æ–‡ä»¶å¦‚ä¸‹ï¼š

```text
Meta-Llama-3-8B
    â”œâ”€â”€ params.json
    â”œâ”€â”€ tokenizer.model
    â””â”€â”€ consolidated.00.pth
```

------

![llama3](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_171617536.png)

# æ„å»ºTransformerçš„è¾“å…¥

## åŠ è½½åˆ†è¯å™¨

> è¿™é‡Œä¸ä¼šå®ç°BPEåˆ†è¯å™¨ï¼Œå¦‚æœå¯¹æ­¤æ„Ÿå…´è¶£ï¼Œå¯ä»¥å‚è€ƒ[mindbpe](https://github.com/karpathy/minbpe)ï¼Œå®ƒæ˜¯ä¸€ä¸ªå¾ˆå¹²å‡€çš„å®ç°ã€‚    

æˆ‘ä»¬ç›´æ¥ä½¿ç”¨OpenAIçš„`tiktoken`ä½œä¸ºBPEåˆ†è¯å™¨ã€‚

### è¯å…¸æ–‡ä»¶

è¯å…¸æ–‡ä»¶æ˜¯`tokenizer.model`ï¼Œæˆ‘ä»¬å…ˆæ‰“å¼€çœ‹ä¸€ä¸‹ï¼š

```text
IQ== 0
Ig== 1
Iw== 2
JA== 3
JQ== 4
...
```

ğŸ˜¶ğŸ˜¶ğŸ˜¶ 

å¯ä»¥çŒœæµ‹å‡ºç¬¬2åˆ—çš„`0, 1, 2,...`è¿™äº›æ˜¯è¯çš„`id`å·ï¼Œä½†æ˜¯ç¬¬1åˆ—æ˜¯è¯å—ï¼Œæ€ä¹ˆå‡ ä¹æ¯ä¸ªè¯éƒ½ä»¥`=`ç»“å°¾ï¼Œè€Œä¸”æ€ä¹ˆå…¨æ˜¯è‹±æ–‡ï¼Ÿä¸­æ–‡å‘¢ï¼Ÿï¼Ÿ

æˆ‘ä»¬ä½¿ç”¨`tiktoken`çš„`load_tiktoken_bpe`ï¼ŒåŠ è½½è¿™ä¸ªè¯å…¸ï¼š

```python
from tiktoken.load import load_tiktoken_bpe

mergeable_ranks = load_tiktoken_bpe("Meta-Llama-3-8B/tokenizer.model")
```

æ‰“å°å‡ºæ¥çœ‹ä¸€ä¸‹ï¼š

```python
{
    b'!': 0,
    b'"': 1, 
    b'#': 2, 
    b'$': 3, 
    b'%': 4
    ...
}
```

å†æŸ¥çœ‹çš„æºç ï¼š

```python
def load_tiktoken_bpe(
    tiktoken_bpe_file: str, expected_hash: Optional[str] = None
) -> dict[bytes, int]:
    # NB: do not add caching to this function
    contents = read_file_cached(tiktoken_bpe_file, expected_hash)
    return {
        base64.b64decode(token): int(rank)
        for token, rank in (line.split() for line in contents.splitlines() if line)
    }
```

ğŸ¤£ğŸ¤£ğŸ¤£åŸæ¥å­—ç¬¦ä¸²"`IQ==`"æ˜¯å­—èŠ‚'`!`'çš„base64ç¼–ç ï¼è€Œ"`=`"æ˜¯base64ç¼–ç çš„è§„èŒƒè¦æ±‚å¡«å……çš„ï¼

çœ‹æ¥è¿™ä¸ªæ˜¯Bytes-Levelçš„BPEï¼ˆByte-Pair Encodingï¼‰åˆ†è¯ï¼Œä»¥1ä¸ªå­—èŠ‚ä¸º1ç§â€œå­—ç¬¦â€ã€‚

è¿™æ ·æœ‰ä¸ªå¥½å¤„ï¼š

å¦‚æœæŒ‰ç…§Unicodeå­—ç¬¦è¡¨è¿›è¡Œåˆ†è¯ï¼Œé‚£è¯æ±‡è¡¨è‚¯å®šååˆ†åºå¤§ï¼Œä½†æ˜¯å¦‚æœæ‹†æˆå­—èŠ‚ï¼Œé‚£å°±åªæœ‰256ä¸ªã€‚

`mergeable_ranks`çš„å‰256ä¸ªå°±æ˜¯å­—èŠ‚åŸºç¡€å­—ç¬¦é›†ï¼Œç¬¬256ä¸ªä¹‹åçš„è¯éƒ½æ˜¯BPEè®­ç»ƒå‡ºæ¥çš„ã€‚

### åˆ†è¯å™¨

å‚è€ƒmetaçš„[llama3](https://github.com/meta-llama/llama3/blob/main/llama/tokenizer.py)ä»£ç ï¼ŒåŠ è½½åˆ†è¯å™¨ï¼Œä»£ç å¦‚ä¸‹ï¼š    

```python
from pathlib import Path
import tiktoken
from tiktoken.load import load_tiktoken_bpe

tokenizer_path = "Meta-Llama-3-8B/tokenizer.model"
special_tokens = [
            "<|begin_of_text|>",
            "<|end_of_text|>",
            "<|reserved_special_token_0|>",
            "<|reserved_special_token_1|>",
            "<|reserved_special_token_2|>",
            "<|reserved_special_token_3|>",
            "<|start_header_id|>",
            "<|end_header_id|>",
            "<|reserved_special_token_4|>",
            "<|eot_id|>",  # end of turn
        ] + [f"<|reserved_special_token_{i}|>" for i in range(5, 256 - 5)]
mergeable_ranks = load_tiktoken_bpe(tokenizer_path)
tokenizer = tiktoken.Encoding(
    name=Path(tokenizer_path).name,
    pat_str=r"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+",
    mergeable_ranks=mergeable_ranks,
    special_tokens={token: len(mergeable_ranks) + i for i, token in enumerate(special_tokens)},
)

print(tokenizer.encode("hello world!")) # [15339, 1917, 0]
print(tokenizer.decode(tokenizer.encode("hello world!"))) # 'hello world!' 
```

ğŸ˜¨ğŸ˜¨ğŸ˜¨

å¯ä»¥çœ‹åˆ°ï¼Œé‡Œé¢ç«Ÿç„¶æœ‰ä¸€å †ä¸çŸ¥é“å¹²å˜›çš„å‚æ•°ï¼

çœ‹æ¥åˆå¾—ç»†ç»†çš„æ‹†å¼€æ¥çœ‹ä¸€ä¸‹ï¼ï¼

#### special_tokens

special_tokenså®šä¹‰äº†ä¸€äº›ç‰¹æ®Štokensï¼Œç”¨ä»¥æ ‡è®°ä¸Šä¸‹æ–‡åœºæ™¯ã€‚ç›´æ¥å»çœ‹[å®˜æ–¹æ–‡æ¡£](https://github.com/meta-llama/llama-recipes)è§£é‡Šï¼š    

| Token                                          | Description                                                  |
| :--------------------------------------------- | ------------------------------------------------------------ |
| <\|begin_of_text\|>                            | Specifies the start of the prompt.                           |
| <\|end_of_text\|>                              | Specifies the end of the prompt. <br />For multiturn-conversations it's usually unused. Instead, every message is terminated with <\|eot_id\|> instead. |
| <\|eot_id\|>                                   | This token signifies the end of the message in a turn i.e. the end of a single message by a system, user or assistant role as shown below. |
| <\|start_header_id\|>{role}<\|end_header_id\|> | These tokens enclose the role for a particular message. The possible roles can be: system, user, assistant. |

Llama 3 çš„å¤šè½®å¯¹è¯éµå¾ªä»¥ä¸‹æç¤ºæ¨¡æ¿ï¼š

```text
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{{ system_prompt }}<|eot_id|><|start_header_id|>user<|end_header_id|>

{{ user_message_1 }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

{{ model_answer_1 }}<|eot_id|><|start_header_id|>user<|end_header_id|>

{{ user_message_2 }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
```

`<|eot_id|>`åœ¨å¼€å§‹æ–°çš„æ ‡å¤´ä¹‹å‰ï¼Œæ¯æ¡æ¶ˆæ¯åé¢éƒ½ä¼šè·Ÿç€ä¸€ä¸ªæ ‡è®°ï¼Œè¡¨ç¤ºè§’è‰²çš„å˜åŒ–ã€‚

æˆ‘ä»¬ä¸¾å‡ ä¸ªä¾‹å­å§ï¼

1. ç³»ç»Ÿprompt

```text
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a helpful AI assistant for travel tips and recommendations<|eot_id|><|start_header_id|>user<|end_header_id|>

What can you help me with?<|eot_id|><|start_header_id|>assistant<|end_header_id|>
```
2. å•ä¸ªç”¨æˆ·æ¶ˆæ¯çš„prompt
```text
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

What is France's capital?<|eot_id|><|start_header_id|>assistant<|end_header_id|>
```
3. ç³»ç»Ÿpromptä»¥åŠç”¨æˆ·å’ŒåŠ©æ‰‹ä¹‹é—´çš„å¤šè½®å¯¹è¯
```text
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a helpful AI assistant for travel tips and recommendations<|eot_id|><|start_header_id|>user<|end_header_id|>

What is France's capital?<|eot_id|><|start_header_id|>assistant<|end_header_id|>

Bonjour! The capital of France is Paris!<|eot_id|><|start_header_id|>user<|end_header_id|>

What can I do there?<|eot_id|><|start_header_id|>assistant<|end_header_id|>

Paris, the City of Light, offers a romantic getaway with must-see attractions like the Eiffel Tower and Louvre Museum, romantic experiences like river cruises and charming neighborhoods, and delicious food and drink options, with helpful tips for making the most of your trip.<|eot_id|><|start_header_id|>user<|end_header_id|>

Give me a detailed list of the attractions I should visit, and time it takes in each one, to plan my trip accordingly.<|eot_id|><|start_header_id|>assistant<|end_header_id|>
```

ä½†æ˜¯ï¼Œé‚£ä¸€å †`reserved_special_token`åˆæ˜¯ä»€ä¹ˆä¸œè¥¿ï¼Œè¿™é‡Œæ²¡è§£é‡Šå•Šï¼

ğŸ˜¨ğŸ˜¨ğŸ˜¨

å»ç¿»é˜…issueï¼Œå‘ç°æœ‰å¥½å¿ƒäººå»é—®äº†ï¼š[Reserved special tokens Â· Issue #77 Â· meta-llama/llama3 Â· GitHub](https://github.com/meta-llama/llama3/issues/77)    

æ€»ç»“ä¸‹æ¥å°±æ˜¯ï¼šæ”¯æŒæ›´å¤šçš„`use-cases`è€Œä¸éœ€è¦è°ƒæ•´è¯è¡¨å¤§å°ã€‚

åœ¨ä¸‹æ¸¸è®­ç»ƒä»»åŠ¡ä¸Šï¼Œç”¨æˆ·æ ¹æ®è‡ªå·±çš„éœ€æ±‚ï¼Œå¯èƒ½éœ€è¦æ·»åŠ ä¸€äº›è‡ªå®šä¹‰çš„tokenï¼Œä½†æ˜¯ä¿®æ”¹è¯è¡¨å¤§å°çš„æˆæœ¬å¤ªé«˜ï¼Œå°±å¯ä»¥æ”¹è¿™é‡Œçš„`reserved_special_token`ã€‚

#### pat_str

æˆ‘ä»¬å…ˆçœ‹çœ‹è¿™æ®µæ­£åˆ™è¡¨è¾¾å¼çš„å«ä¹‰ï¼Œè®©ChatGPTè§£é‡Šä¸€ä¸‹ã€‚

------
æ ¹æ®`|`å¯ä»¥åˆ†ä¸º7éƒ¨åˆ†ï¼š

1. `(?i:'s|'t|'re|'ve|'m|'ll|'d)`
   - `(?i:)` æ˜¯ä¸€ä¸ªéæ•è·ç»„ï¼Œå¹¶ä¸” `i` è¡¨ç¤ºå¿½ç•¥å¤§å°å†™ã€‚
   - è¿™éƒ¨åˆ†åŒ¹é…å¸¸è§çš„è‹±æ–‡ç¼©ç•¥å½¢å¼ï¼Œå¦‚ `'s`ï¼ˆis æˆ– hasï¼‰ï¼Œ`'t`ï¼ˆnotï¼‰ï¼Œ`'re`ï¼ˆareï¼‰ï¼Œ`'ve`ï¼ˆhaveï¼‰ï¼Œ`'m`ï¼ˆamï¼‰ï¼Œ`'ll`ï¼ˆwillï¼‰ï¼Œå’Œ `â€™d`ï¼ˆhad æˆ– wouldï¼‰ã€‚

2. `[^\r\n\p{L}\p{N}]?\p{L}+`
   - `[^\r\n\p{L}\p{N}]?` åŒ¹é…ä¸€ä¸ªå¯é€‰çš„éå­—æ¯ã€éæ•°å­—ã€éå›è½¦ã€éæ¢è¡Œç¬¦çš„å­—ç¬¦ã€‚
   - `\p{L}+` åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ª Unicode å­—ç¬¦é›†ä¸­çš„å­—æ¯å­—ç¬¦ã€‚

3. `\p{N}{1,3}`
   - `\p{N}{1,3}` åŒ¹é… 1 åˆ° 3 ä¸ª Unicode æ•°å­—å­—ç¬¦ã€‚

4. ` ?[^\s\p{L}\p{N}]+[\r\n]*`
   - ` ?` åŒ¹é…ä¸€ä¸ªå¯é€‰çš„ç©ºæ ¼ã€‚
   - `[^\s\p{L}\p{N}]+` åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ªéç©ºç™½ã€éå­—æ¯ã€éæ•°å­—çš„å­—ç¬¦ã€‚
   - `[\r\n]*` åŒ¹é…é›¶ä¸ªæˆ–å¤šä¸ªå›è½¦æˆ–æ¢è¡Œç¬¦ã€‚

5. `\s*[\r\n]+`
   - `\s*` åŒ¹é…é›¶ä¸ªæˆ–å¤šä¸ªç©ºç™½å­—ç¬¦ã€‚
   - `[\r\n]+` åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ªå›è½¦æˆ–æ¢è¡Œç¬¦ã€‚

6. `\s+(?!\S)`
   - `\s+` åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ªç©ºç™½å­—ç¬¦ã€‚
   - `(?!\S)` æ˜¯ä¸€ä¸ªè´Ÿå‘å‰ç»ï¼Œç¡®ä¿åé¢ä¸æ˜¯ä¸€ä¸ªéç©ºç™½å­—ç¬¦ã€‚

7. `\s+`
   - åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ªç©ºç™½å­—ç¬¦ã€‚


è¿™æ®µæ­£åˆ™è¡¨è¾¾å¼ä¸»è¦ç”¨äºåˆ†è¯ï¼Œå®ƒèƒ½å¤Ÿå¤„ç†ä»¥ä¸‹å†…å®¹ï¼š
- å¸¸è§è‹±æ–‡ç¼©ç•¥å½¢å¼
- å•è¯ï¼ˆç”±å­—æ¯ç»„æˆï¼‰
- çŸ­çš„æ•°å­—åºåˆ—ï¼ˆ1 åˆ° 3 ä¸ªæ•°å­—ï¼‰
- ç‰¹æ®Šç¬¦å·å’Œæ ‡ç‚¹
- ç©ºç™½å­—ç¬¦å’Œæ¢è¡Œç¬¦

------

æ‰€ä»¥ï¼Œè¿™æ®µæ­£åˆ™è¡¨è¾¾å¼å…¶å®æ˜¯ä¸€ä¸ªâ€œæ­£åˆ™åˆ†è¯å™¨â€ï¼Œé€‚ç”¨äºå¤šç§è¯­è¨€çš„æ–‡æœ¬å¤„ç†ï¼Œç‰¹åˆ«æ˜¯åŒ…å«å¤šç§å­—ç¬¦é›†å’Œç¬¦å·çš„æ–‡æœ¬ã€‚

ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰


æ€»ç»“æ¥çœ‹ï¼Œè¿™ä¸ªtokenizeråˆ†è¯å™¨åŒ…æ‹¬3éƒ¨åˆ†ï¼š
1. ç‰¹æ®Šå¤„ç†æ–‡æœ¬ä¸­çš„special_tokens
2. ç”¨æ­£åˆ™è¡¨è¾¾å¼å¯¹æ–‡æœ¬è¿›è¡Œåˆ‡åˆ†
3. åŸºäºBPEè¯å…¸å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯

## è¯»å–æ¨¡å‹æ–‡ä»¶

è®©æˆ‘ä»¬å…ˆè¯»å–æ¨¡å‹æƒé‡æ–‡ä»¶çœ‹ä¸€ä¸‹ï¼š

å› ä¸ºæ¨¡å‹æƒé‡æ–‡ä»¶é‡‡ç”¨çš„pytorchæ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨`torch.load`åŠ è½½å³å¯ï¼š

```python
model = torch.load("Meta-Llama-3-8B/consolidated.00.pth")
print(json.dumps(list(model.keys())[:20], indent=4))
```
æ‰“å°å‡ºæ¥ï¼Œå¯ä»¥çœ‹åˆ°å®ƒæ˜¯ç¥ç»ç½‘ç»œæ¯ä¸€å±‚çš„æƒé‡çŸ©é˜µ

```python
[
    "tok_embeddings.weight",
    "layers.0.attention.wq.weight",
    "layers.0.attention.wk.weight",
    "layers.0.attention.wv.weight",
    "layers.0.attention.wo.weight",
    "layers.0.feed_forward.w1.weight",
    "layers.0.feed_forward.w3.weight",
    "layers.0.feed_forward.w2.weight",
    "layers.0.attention_norm.weight",
    "layers.0.ffn_norm.weight",
    "layers.1.attention.wq.weight",
    "layers.1.attention.wk.weight",
    "layers.1.attention.wv.weight",
    "layers.1.attention.wo.weight",
    "layers.1.feed_forward.w1.weight",
    "layers.1.feed_forward.w3.weight",
    "layers.1.feed_forward.w2.weight",
    "layers.1.attention_norm.weight",
    "layers.1.ffn_norm.weight",
    "layers.2.attention.wq.weight"
]
```

å†çœ‹ä¸€ä¸‹æ¨¡å‹é…ç½®æ–‡ä»¶ï¼š

```python
with open("Meta-Llama-3-8B/params.json", "r") as f:
    config = json.load(f)
print(config)
```

æ‰“å°å‡ºæ¥ï¼Œå¾—åˆ°æ¨¡å‹æ¨¡å‹çš„ç»“æ„å‚æ•°ï¼š

```python
{'dim': 4096,
 'n_layers': 32,
 'n_heads': 32,
 'n_kv_heads': 8,
 'vocab_size': 128256,
 'multiple_of': 1024,
 'ffn_dim_multiplier': 1.3,
 'norm_eps': 1e-05,
 'rope_theta': 500000.0}
```

æˆ‘ä»¬ä½¿ç”¨æ­¤é…ç½®æ¥æ¨æ–­æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯ï¼š

1. dimï¼Œéšè—å±‚å‘é‡çš„é•¿åº¦ä¸º4096
2. n_layersï¼Œæ¨¡å‹æœ‰32ä¸ªTransformerå±‚
3. n_headsï¼Œæ¯ä¸ªæ³¨æ„åŠ›æ¨¡å—æœ‰32ä¸ªå¤´
4. n_kv_headsï¼Œæ¯ä¸ªæ³¨æ„åŠ›æ¨¡å—æœ‰8ä¸ªKeyå’ŒValueå¤´
5. vocab_sizeï¼Œè¯æ±‡å¤§å°ä¸º128256
6. multiple_ofï¼Œ
7. ffn_dim_multiplierï¼Œ
8. norm_epsï¼Œnormçš„é˜²æº¢å‡ºepså€¼ä¸º1e-05
9. rope_thetaï¼ŒRoPEä½ç½®ç¼–ç çš„baseå€¼

```python
dim = config["dim"]
n_layers = config["n_layers"]
n_heads = config["n_heads"]
n_kv_heads = config["n_kv_heads"]
vocab_size = config["vocab_size"]
multiple_of = config["multiple_of"]
ffn_dim_multiplier = config["ffn_dim_multiplier"]
norm_eps = config["norm_eps"]
rope_theta = torch.tensor(config["rope_theta"])
```

## ç¼–ç ï¼šæ–‡æœ¬è½¬tokens

![tokens](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_214612456.png)

æˆ‘ä»¬ä½¿ç”¨ä¸Šé¢åŠ è½½çš„åˆ†è¯å™¨`tokenizer`ï¼Œå°†ä¸€æ®µæ–‡æœ¬è½¬ä¸ºtokensï¼Œé•¿åº¦ä¸º17ã€‚

```python
prompt = "the answer to the ultimate question of life, the universe, and everything is "
# 128000å¯¹åº”tokenä¸º<|begin_of_text|>ï¼Œç”¨æ¥æ ‡è®°æ–‡æœ¬çš„å¼€å§‹
tokens = [128000] + tokenizer.encode(prompt)
print(tokens)
tokens = torch.tensor(tokens)
prompt_split_as_tokens = [tokenizer.decode([token.item()]) for token in tokens] # [17]
print(prompt_split_as_tokens)
```

```python
[128000, 1820, 4320, 311, 279, 17139, 3488, 315, 2324, 11, 279, 15861, 11, 323, 4395, 374, 220]
['<|begin_of_text|>', 'the', ' answer', ' to', ' the', ' ultimate', ' question', ' of', ' life', ',', ' the', ' universe', ',', ' and', ' everything', ' is', ' ']
```

## tokensè½¬embedding

![embeddings](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_221650056.png)

Embeddingæ˜¯ç¥ç»ç½‘ç»œæ¨¡å—çš„ä¸€éƒ¨åˆ†ï¼Œä¸éœ€è¦ç”¨æˆ·å®ç°ï¼Œè¿™é‡Œç›´æ¥è°ƒç”¨`torch.nn.Embedding`å³å¯ã€‚

```python
# åˆå§‹åŒ–Embeddingæ¨¡å—ï¼Œå¯¹åº”çš„embeddingçŸ©é˜µShapeä¸º[vocab_size, dim]=[128256, 4096]
embedding_layer = torch.nn.Embedding(vocab_size, dim)
# ç”¨modelæƒé‡å˜é‡ï¼Œç»™embeddingçŸ©é˜µèµ‹å€¼
embedding_layer.weight.data.copy_(model["tok_embeddings.weight"]) # [128256, 4096]
# å°†tokensè½¬ä¸ºembeddingå‘é‡ï¼Œå…¶å®å°±æ˜¯ä»¥tokensä¸ºè¡Œç´¢å¼•ï¼Œå»æŠ½å–embeddingçŸ©é˜µï¼Œ[17] -> [17, 4096]
token_embeddings_unnormalized = embedding_layer(tokens).to(torch.bfloat16) # [17, 4096]
```

# æ„å»º Transformer çš„ç¬¬ä¸€å±‚

![Step of Transformer Block](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_222241997.png)

## ä½¿ç”¨RMSå¯¹embeddingå½’ä¸€åŒ–

![Step of rms_norm](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_222442163.png)

æˆ‘ä»¬å¹³æ—¶åšå½’ä¸€åŒ–ï¼Œæœ‰ç§å¸¸è§çš„æ–¹æ³•ï¼Œå°±æ˜¯"å‡å»å‡å€¼ï¼Œé™¤ä»¥æ–¹å·®"ï¼š
$$
\bar{a}_i = \frac{a_i - \mu}{\sigma}
$$

$$
\mu =\frac{1}{n}\sum_{i=1}^n{a_{i}}\quad
$$
$$
\sigma =\sqrt{\frac{1}{n}\sum_{i=1}^n{\left( a_i-\mu \right)}^2}
$$



è€Œè¿™é‡Œçš„`RMSNorm`è®¤ä¸ºï¼Œ`re-centering invariance property`æ˜¯ä¸å¿…è¦çš„ï¼Œåªç”¨ä¿ç•™`re-scaling invariance property`ï¼Œæ‰€ä»¥å°±æŠŠä¸Šè¿°çš„å‡å€¼å»æ‰äº†ã€‚

æœ€ç»ˆï¼Œ`RMSNorm`çš„è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼Œå…¶ä¸­$g_{i}$æ˜¯ç¼©æ”¾å› å­ï¼Œé€šè¿‡è®­ç»ƒå¾—åˆ°ã€‚
$$
\bar{a}_i=\frac{a_i}{\mathbf{RMS}\left( \mathbf{a} \right)}g_i,\quad \mathbf{where\,\,RMS}\left( \mathbf{a} \right) =\sqrt{\frac{1}{n}\sum_{i=1}^n{a_{i}^{2}}}
$$

ä¸ºäº†é˜²æ­¢åˆ†æ¯ä¸º0ï¼Œè¿˜ä¼šåŠ ä¸€ä¸ªæå°å€¼norm_epsã€‚
```python
# def rms_norm(tensor, norm_weights):
#     rms = (tensor.pow(2).mean(-1, keepdim=True) + norm_eps)**0.5
#     return tensor * (norm_weights / rms)
def rms_norm(tensor, norm_weights):
    return (tensor * torch.rsqrt(tensor.pow(2).mean(-1, keepdim=True) + norm_eps)) * norm_weights
```
å¯¹è¾“å…¥embeddingå½’ä¸€åŒ–ï¼Œå…¶ä¸­çš„ç¼©æ”¾å› å­`norm_weights`ä»modelæƒé‡å˜é‡ä¸­è¯»å–ï¼š

```python
token_embeddings = rms_norm(token_embeddings_unnormalized, model["layers.0.attention_norm.weight"]) # [17, 4096]
```

æ³¨æ„ï¼Œå½’ä¸€åŒ–ä¹‹åç»´åº¦ä¿æŒä¸å˜`[17, 4096]`ã€‚

## å®ç°Attention

![Step of attention](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_222632635.png)

æˆ‘ä»¬å…ˆå›å¿†ä¸€ä¸‹ï¼ŒSelf-Attentionçš„è®¡ç®—å…¬å¼ï¼š
$$
\text{Attention}\left( Q,K,V \right) \ =\ \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right) V
$$
### åŠ è½½æƒé‡

è®©æˆ‘ä»¬åŠ è½½ Transformer ç¬¬ä¸€å±‚çš„æ³¨æ„åŠ›å¤´çš„æƒé‡çœ‹ä¸€ä¸‹ï¼š

```python
print(
    model["layers.0.attention.wq.weight"].shape,
    model["layers.0.attention.wk.weight"].shape,
    model["layers.0.attention.wv.weight"].shape,
    model["layers.0.attention.wo.weight"].shape
)
```
`query`, `key`, `value`, `output`çš„æƒé‡Wå½¢çŠ¶åˆ†åˆ«ä¸ºï¼š

```python
torch.Size([4096, 4096]) torch.Size([1024, 4096]) torch.Size([1024, 4096]) torch.Size([4096, 4096])
```

é€šè¿‡æ¨¡å‹å‚æ•°å¯çŸ¥ï¼Œè¿™ä¸ªAttentionæ¨¡å—æ˜¯ä¸€ä¸ªå¤šå¤´æ³¨æ„åŠ›ï¼Œåº”è¯¥æœ‰`n_heads=32`ä¸ªå¤´ï¼Œé‚£ä¹ˆå°±åº”è¯¥æœ‰32ä¸ª`Wq`æ‰å¯¹ï¼Œä½†æ˜¯è¿™é‡Œåªæœ‰1ä¸ª`Wq`çŸ©é˜µï¼Œå®Œå…¨çœ‹ä¸åˆ°32è¿™ä¸ªå€¼åœ¨å“ªï¼Œ`Wk`ï¼Œ`Wv`,  `Wo`ä¹ŸåŒç†ã€‚

è¿™æ˜¯å› ä¸ºï¼Œä»£ç çš„ä½œè€…æŠŠå®ƒä»¬åˆå¹¶åˆ°1ä¸ªçŸ©é˜µé‡Œé¢äº†ï¼Œæœ‰åŠ©äºå¹¶è¡ŒåŒ–æ³¨æ„åŠ›å¤´çš„çŸ©é˜µä¹˜æ³•è¿ç®—ã€‚

### Query

![query](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_223346892.png)

#### å±•å¼€Wq

å…ˆæŠŠå¤šå¤´æ³¨æ„åŠ›çš„æƒé‡`Wq`å±•å¼€ï¼š

```python
q_layer0 = model["layers.0.attention.wq.weight"]
head_dim = q_layer0.shape[0] // n_heads
q_layer0 = q_layer0.view(n_heads, head_dim, dim) # [32, 128, 4096]
```

å¾—åˆ°çš„å½¢çŠ¶æ˜¯`[32, 128, 4096]`ï¼Œ32æ˜¯llama3ä¸­æ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼Œ128æ˜¯æŸ¥è¯¢å‘é‡çš„å¤§å°ï¼Œ4096æ˜¯token embeddingçš„å¤§å°ã€‚

#### è®¡ç®—Query

![Q](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_223418146.png)

1. å…ˆæ‹¿åˆ°ä¸€ä¸ªå¤´çš„`Wq`ï¼Œå½¢çŠ¶ä¸º`[128, 4096]`:

```python
# ä»¥ç¬¬1ä¸ªå¤´ä¸ºä¾‹
q_layer0_head0 = q_layer0[0] # [128, 4096]
```

2. `Wq`ä¸ token embeddingç›¸ä¹˜ï¼Œå¾—åˆ°tokençš„query

```python
q_per_token = torch.matmul(token_embeddings, q_layer0_head0.T) # [17, 128]
```

å¯ä»¥çœ‹åˆ°ï¼Œç»“æœçš„å½¢çŠ¶æ˜¯ `[17, 128]`ï¼Œè¿™æ˜¯å› ä¸ºæˆ‘ä»¬æœ‰ 17 ä¸ªtokenï¼Œæ¯ä¸ªtokençš„queryå‘é‡é•¿åº¦ä¸º128ã€‚

### ä½ç½®ç¼–ç 

è‡³æ­¤ï¼Œpromptå¯¹åº”çš„æ¯ä¸ªtokenï¼Œéƒ½æœ‰1ä¸ªå¯¹åº”çš„queryå‘é‡ï¼Œä½†æ˜¯å¦‚æœä»”ç»†æƒ³ä¸€ä¸‹å°±ä¼šå‘ç°ï¼Œqueryå‘é‡åœ¨promptçš„é¡ºåºä½ç½®ï¼Œå¥½åƒæ²¡æ³•è¡¨è¾¾å‡ºæ¥ã€‚

ä¾‹å¦‚ï¼š"the answer to the ultimate question of life, the universe, and everything is "ã€‚

åœ¨è¿™ä¸ªpromptä¸­ï¼Œâ€œtheâ€å‡ºç°äº†3æ¬¡ï¼Œå®ƒä»¬çš„ä½ç½®ä¸åŒï¼Œä½†æ˜¯ä¸Šè¿°å¾—åˆ°çš„queryå‘é‡è‚¯å®šæ˜¯ç›¸åŒçš„ï¼Œè¿™å¯¼è‡´æ²¡åŠæ³•ä»queryå‘é‡ä¸ŠåŒºåˆ†å®ƒä»¬ã€‚

æ‰€ä»¥ï¼Œæˆ‘ä»¬éœ€è¦å¼•å…¥RoPE(Rotory Positional Embedding)ä½ç½®ç¼–ç ï¼Œæ ‡è®°tokenåœ¨åºåˆ—ä¸­çš„ä½ç½®ï¼Œè®©è¿™3ä¸ªqueryå‘é‡æœ‰æ‰€ä¸åŒã€‚

#### RoPE

å¯ä»¥çœ‹ä¸€ä¸‹è¿™æ®µè§†é¢‘ï¼Œå­¦ä¹ RoPEçš„æ•°å­¦åŸç†ã€‚https://www.youtube.com/watch?v=o29P0Kpobz0&t=530s    

![RoPE](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_223538448.png)

ç®€å•æ¥è¯´ï¼Œå°±æ˜¯æ ¹æ®tokençš„ä½ç½®ï¼Œåœ¨å¤å¹³é¢ä¸ŠæŠŠqueryå‘é‡æ—‹è½¬æŸä¸ªè§’åº¦ã€‚
$$
f_{\{q,k\}}(\boldsymbol{q}_m,m)=R_{\Theta,m}^dW_{\{q,k\}}\boldsymbol{q}_m
$$

$$
\boldsymbol{R}_{\Theta,m}^d=\begin{pmatrix}\cos m\theta_1&-\sin m\theta_1&0&0&\cdots&0&0\\\sin m\theta_1&\cos m\theta_1&0&0&\cdots&0&0\\0&0&\cos m\theta_2&-\sin m\theta_2&\cdots&0&0\\0&0&\sin m\theta_2&\cos m\theta_2&\cdots&0&0\\\vdots&\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\0&0&0&0&\cdots&\cos m\theta_{d/2}&-\sin m\theta_{d/2}\\0&0&0&0&\cdots&\sin m\theta_{d/2}&\cos m\theta_{d/2}\end{pmatrix}
$$

å¯¹äºå‘é‡çš„æ—‹è½¬æ“ä½œï¼Œæ—¢å¯ä»¥ä½¿ç”¨å·¦ä¹˜æ—‹è½¬çŸ©é˜µRï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨åœ¨å¤å¹³é¢ä¸Šä¹˜æ—‹è½¬è§’åº¦ï¼Œä¸ºäº†æ–¹ä¾¿ç†è§£ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤æ•°çš„æ–¹å¼è¡¨è¾¾ã€‚

ğŸ˜–ğŸ˜–ğŸ˜–

æˆ‘ä»¬ä¸€æ­¥æ­¥æ¥å®ç°è¿™ä¸ªè¿‡ç¨‹ï¼

##### è®¡ç®—æ—‹è½¬è§’åº¦

ä¸ºäº†è¡¨ç¤ºå¤å¹³é¢ä¸Šçš„å‘é‡ï¼Œæˆ‘ä»¬éœ€è¦æŠŠqueryå‘é‡ä¸¤ä¸¤åˆ†ä¸ºä¸€ç»„ï¼Œåˆ†åˆ«ä½œä¸ºå®éƒ¨å’Œè™šéƒ¨ï¼š

```python
q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2) # [17, 64, 2]
```

æˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªå½¢çŠ¶ä¸º`[17, 64, 2]`çš„å‘é‡ï¼Œå®ƒå°†æ¯ä¸ªtokençš„queryå‘é‡åˆ†æˆ64ä¸ªpairsï¼

æ¯ä¸ªpairå¯¹åº”å¤å¹³é¢ä¸Šçš„ä¸€ä¸ªå‘é‡ï¼Œæˆ‘ä»¬éœ€è¦æ—‹è½¬æ¯ä¸ªpairsï¼Œæ—‹è½¬è§’åº¦ä¸ºï¼š$m\theta_{i}$ï¼Œå…¶ä¸­$m$æ˜¯è¯¥tokençš„ä½ç½®ï¼Œ$\theta_{i}$æ˜¯ç¬¬$i$ä¸ªpairçš„æ—‹è½¬è§’åº¦ã€‚

$\theta_{i}$çš„è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š
$$
\theta_i=base^{-2i/d},i\in[0,1,...,d/2-1]
$$


```python
# è®¡ç®—2i/dï¼Œdå°±æ˜¯queryå‘é‡çš„é•¿åº¦128ï¼Œd/2å°±æ˜¯64
zero_to_one_split_into_64_parts = torch.tensor(range(64)) / 64 # [64]
# rope_thetaå°±æ˜¯baseï¼Œè¿™ä¸ªå€¼å¯¹äºæ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦å¤–æ¨èƒ½åŠ›æœ‰æå¤§å½±å“
freqs = 1.0 / (rope_theta ** zero_to_one_split_into_64_parts) # [64]
print(freqs)
```

```python
tensor([1.0000e+00, 8.1462e-01, 6.6360e-01, 5.4058e-01, 4.4037e-01, 3.5873e-01,
        2.9223e-01, 2.3805e-01, 1.9392e-01, 1.5797e-01, 1.2869e-01, 1.0483e-01,
        8.5397e-02, 6.9566e-02, 5.6670e-02, 4.6164e-02, 3.7606e-02, 3.0635e-02,
        2.4955e-02, 2.0329e-02, 1.6560e-02, 1.3490e-02, 1.0990e-02, 8.9523e-03,
        7.2927e-03, 5.9407e-03, 4.8394e-03, 3.9423e-03, 3.2114e-03, 2.6161e-03,
        2.1311e-03, 1.7360e-03, 1.4142e-03, 1.1520e-03, 9.3847e-04, 7.6450e-04,
        6.2277e-04, 5.0732e-04, 4.1327e-04, 3.3666e-04, 2.7425e-04, 2.2341e-04,
        1.8199e-04, 1.4825e-04, 1.2077e-04, 9.8381e-05, 8.0143e-05, 6.5286e-05,
        5.3183e-05, 4.3324e-05, 3.5292e-05, 2.8750e-05, 2.3420e-05, 1.9078e-05,
        1.5542e-05, 1.2660e-05, 1.0313e-05, 8.4015e-06, 6.8440e-06, 5.5752e-06,
        4.5417e-06, 3.6997e-06, 3.0139e-06, 2.4551e-06])
```
è®¡ç®—æ—‹è½¬è§’åº¦$m\theta_{i}$ï¼š

```python
# è®¡ç®—æ—‹è½¬è§’åº¦
freqs_for_each_token = torch.outer(torch.arange(17), freqs) # [17, 64]
# å°†è§’åº¦è½¬ä¸ºå¤æ•°
freqs_cis = torch.polar(torch.ones_like(freqs_for_each_token), freqs_for_each_token) # [17, 64]

# viewing tjhe third row of freqs_cis
import matplotlib.pyplot as plt

value = freqs_cis[3]
plt.figure()
for i, element in enumerate(value[:17]):
    plt.plot([0, element.real], [0, element.imag], color='blue', linewidth=1, label=f"Index: {i}")
    plt.annotate(f"{i}", xy=(element.real, element.imag), color='red')
plt.xlabel('Real')
plt.ylabel('Imaginary')
plt.title('Plot of one row of freqs_cis')
plt.show()
```

![freqs](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_224820074.png)

##### æ—‹è½¬Query

![Rotation matrix](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_223808896.png)

æˆ‘ä»¬å¯ä»¥å°†query pairsè½¬æ¢ä¸ºå¤æ•°ï¼Œç„¶åä½¿ç”¨ç‚¹ç§¯æ¥æ—‹è½¬ï¼š

```python
# query pairsè½¬ä¸ºå¤æ•°
q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs) # [17, 64]
# åœ¨å¤å¹³é¢ä¸Šquery pairsä¹˜ä»¥æ—‹è½¬è§’åº¦ï¼Œå®ç°å‘é‡çš„æ—‹è½¬
q_per_token_as_complex_numbers_rotated = q_per_token_as_complex_numbers * freqs_cis # [17, 64]
```

```python
# æŠŠæ—‹è½¬query pairsæ—‹è½¬ç»“æœå†è½¬ä¸ºå®æ•°
q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers_rotated) # [17, 64, 2]
# å±•å¹³query pairsï¼Œä»è€ŒæŠŠqueryçš„å½¢çŠ¶æ¢å¤å›å»
q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape) # [17, 128]
```

è‡³æ­¤ï¼Œæˆ‘ä»¬å¾—åˆ°äº†å¸¦æœ‰ä½ç½®ç¼–ç ä¿¡æ¯çš„queryå‘é‡

### Key

![Key](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_225028384.png)

æ ¹æ®Qä¸KVæ•°é‡çš„å¯¹åº”å…³ç³»ï¼Œåˆ†ä¸º`1 : 1`ï¼Œ`n : m`, `n : 1`ï¼Œåˆ†åˆ«å«åšMHAï¼ˆMulti-head Attentionï¼‰ï¼ŒGQAï¼ˆGrouped-Query Attentionï¼‰ï¼ŒMQAï¼ˆMulti-Query Attentionï¼‰ã€‚

![GQA](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_225144272.png)

keyçš„è®¡ç®—è¿‡ç¨‹ä¸queryçš„è®¡ç®—è¿‡ç¨‹å‡ ä¹ç›¸åŒï¼Œå”¯ä¸€çš„åŒºåˆ«æ˜¯keyçš„æ•°é‡å’Œqueryä¸åŒã€‚

- ç”±äº`n_heads=32`ï¼Œ`n_kv_heads=8`ï¼Œqueryå¤´æ˜¯32ä¸ªï¼Œkeyå¤´æ˜¯8ä¸ªï¼Œé‚£ä¹ˆæ¯4ä¸ªqueryå¤´å…±äº«1ä¸ªkeyå¤´ï¼Œç›®çš„æ˜¯å‡å°‘æ‰€éœ€çš„è®¡ç®—æ¬¡æ•°ã€‚

- keyå‘é‡ä¹Ÿæ˜¯128ç»´
- keyå‘é‡ä¹Ÿéœ€è¦æ·»åŠ ä½ç½®ä¿¡æ¯

#### å±•å¼€Wk

```python
# åŠ è½½Wk
k_layer0 = model["layers.0.attention.wk.weight"]
# æœ‰8ä¸ªkeyå¤´çš„Wkï¼Œæ¯ä¸ªå¤´çš„Wkå½¢çŠ¶æ˜¯[128, 4096]
k_layer0 = k_layer0.view(n_kv_heads, k_layer0.shape[0] // n_kv_heads, dim) # [8, 128, 4096]
# ä»¥ç¬¬1ä¸ªå¤´ä¸ºä¾‹
k_layer0_head0 = k_layer0[0] # [128, 4096]
```
#### è®¡ç®—Key

![K](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_225311078.png)

```python
# è®¡ç®—ä¸å¸¦ä½ç½®ä¿¡æ¯çš„keyå‘é‡
k_per_token = torch.matmul(token_embeddings, k_layer0_head0.T) # [17, 128]
```

å¯¹keyå‘é‡æ—‹è½¬ï¼Œä»è€Œæ·»åŠ ä½ç½®ä¿¡æ¯ï¼š


```python
# ä¸¤ä¸¤ä¸€ç»„ï¼Œè½¬ä¸ºkey pairs
k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2) # [17, 64, 2]
# key pairsä½œä¸ºå¤æ•°çš„å®éƒ¨å’Œè™šéƒ¨ï¼Œå°†key pairsè½¬ä¸ºå¤å¹³é¢ä¸Šçš„å‘é‡
k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs) # [17, 64]
# åœ¨å¤å¹³é¢ä¸Škey pairsä¹˜ä»¥æ—‹è½¬è§’åº¦ï¼Œå®ç°å‘é‡çš„æ—‹è½¬
k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis) # [17, 64, 2]
# å±•å¹³key pairsï¼Œä»è€ŒæŠŠkeyå‘é‡çš„å½¢çŠ¶æ¢å¤å›å»
k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape) # [17, 128]
```

keyçš„å½¢çŠ¶ä¸queryç›¸åŒï¼Œéƒ½æ˜¯[17, 128]ï¼Œæœ‰17ä¸ªtokenï¼Œæ¯ä¸ªtokençš„keyå‘é‡é•¿åº¦ä¸º128ã€‚

### Attention Map

![Attention Map](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_225426284.png)

è¿™ä¸€æ­¥è®¡ç®—Attentionå…¬å¼ä¸­çš„$\text{softmax}(Q*K^T/\sqrt{d_k})$éƒ¨åˆ†ï¼Œå¯ä»¥å¾—åˆ°ä¸€ä¸ªattention mapï¼Œå®ƒæè¿°äº†æ¯ä¸ªtokençš„ä¸å®ƒä¹‹å‰æ‰€æœ‰tokençš„æ¦‚ç‡ä¾èµ–ç¨‹åº¦ã€‚

#### Queryä¹˜Key

å…ˆè®¡ç®—$Q*K^T/\sqrt{d_k}$ï¼š

```python
qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(head_dim)**0.5 # [17, 17]
```

è¿™ä¼šå¾—åˆ°ä¸€ä¸ªscore mapï¼Œæè¿°äº†æ¯ä¸ªtokençš„queryä¸æ¯ä¸ªtokençš„keyçš„å…³è”ç¨‹åº¦ï¼Œè¿™å°±æ˜¯Self-Attentionã€‚

æ¥å¯è§†åŒ–çœ‹ä¸€ä¸‹å®ƒï¼š

```python
def display_qk_heatmap(qk_per_token):
    _, ax = plt.subplots()
    im = ax.imshow(qk_per_token.to(float).detach(), cmap='viridis')
    ax.set_xticks(range(len(prompt_split_as_tokens)))
    ax.set_yticks(range(len(prompt_split_as_tokens)))
    ax.set_xticklabels(prompt_split_as_tokens, rotation=45, rotation_mode='anchor', ha="right", va="center")
    ax.set_yticklabels(prompt_split_as_tokens)
    ax.figure.colorbar(im, ax=ax)
    
display_qk_heatmap(qk_per_token)
```

![Map](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_225549700.png)

#### Causal mask

å¯¹äºæ¨¡å‹è®­ç»ƒï¼Œè¿™é‡Œæœ‰ä¸ªé—®é¢˜ï¼š

åœ¨è®­ç»ƒæœŸé—´ï¼Œæˆ‘ä»¬éœ€è¦ç”¨è¿‡å»çš„tokenå»é¢„æµ‹æœªæ¥çš„tokenï¼Œä½†æ˜¯ä¸Šè¿°score mapä¼šæŠŠæœªæ¥çš„æ³¨æ„åŠ›ä¹Ÿç®—è¿›å»ã€‚

ä¾‹å¦‚ï¼š

å¯¹äºç¬¬15ä¸ªtokenï¼Œæˆ‘ä»¬çš„è®­ç»ƒç›®æ ‡æ˜¯æŠŠç¬¬1~15ä¸ªtokenä½œä¸ºä¸Šæ–‡ï¼Œå»é¢„æµ‹ç¬¬16ä¸ªtokenã€‚

ç„¶è€Œï¼Œç«™åœ¨ç¬¬15ä¸ªtokenä¸Šï¼Œä»å› æœè®ºçš„è§’åº¦çœ‹ï¼Œç¬¬16~17ä¸ªtokenæ˜¯æœªæ¥çš„é¢„æµ‹ç›®æ ‡ï¼Œç°åœ¨è¿˜ä¸å­˜åœ¨å‘¢ï¼Œæ‰€ä»¥ç¬¬1~15ä¸ªtokenä¸ç¬¬16~17ä¸ªtokenæ˜¯æ²¡æœ‰æ³¨æ„åŠ›çš„ã€‚

äºæ˜¯ï¼Œæˆ‘ä»¬åœ¨è®¡ç®—attentionçš„æ—¶å€™ï¼Œéœ€è¦å±è”½æ‰æœªæ¥çš„tokenã€‚

æ€ä¹ˆå±è”½å‘¢ï¼Ÿå¯ä»¥å‘ç°ï¼š

- å¯¹äºç¬¬1ä¸ªtokenï¼Œéœ€è¦å±è”½ç¬¬2~17ä¸ªtoken
- å¯¹äºç¬¬2ä¸ªtokenï¼Œéœ€è¦å±è”½ç¬¬3~17ä¸ªtoken
- ...
- å¯¹äºç¬¬16ä¸ªtokenï¼Œéœ€è¦å±è”½ç¬¬17ä¸ªtoken
- å¯¹äºç¬¬17ä¸ªtokenï¼Œä¸éœ€è¦å±è”½

è¿™æ ·çœ‹æ¥ï¼Œå¯¹äºè¿™ä¸ª17*17çš„score mapï¼Œéœ€è¦å±è”½çŸ©é˜µçš„ä¸Šä¸‰è§’éƒ¨åˆ†ã€‚

é‚£ä¹ˆæˆ‘ä»¬åªéœ€è¦æ„é€ ä¸€ä¸ªä¸Šä¸‰è§’maskï¼Œç”¨å®ƒæ¥å±è”½æ‰score mapå¯¹åº”çš„ä½ç½®ï¼Œæˆ‘ä»¬ä¸€èˆ¬ç§°å®ƒä¸ºcausal_maskã€‚

```python
mask = torch.full((len(tokens), len(tokens)), float("-inf"), device=tokens.device)
mask = torch.triu(mask, diagonal=1) # [17, 17]
print(mask)
```

```python
tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
```

```python
qk_per_token_after_masking = qk_per_token + mask  # [17, 17]
display_qk_heatmap(qk_per_token_after_masking)
```

![mask](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_225632341.png)


#### Softmax

softmaxä¸»è¦æœ‰2ä¸ªä½œç”¨ï¼š

- å½’ä¸€åŒ–å¾—åˆ°æ‰€æœ‰æƒé‡ç³»æ•°ä¹‹å’Œä¸º1çš„æ¦‚ç‡åˆ†å¸ƒ
- ç”¨softmaxå‡½æ•°çš„ç‰¹æ€§çªå‡ºé‡è¦å…ƒç´ çš„æƒé‡

```python
qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)  # [17, 17]
display_qk_heatmap(qk_per_token_after_masking_after_softmax)
```

![softmax](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_225723272.png)

è‡³æ­¤ï¼Œæˆ‘ä»¬å¾—åˆ°äº†è¿™ä¸ªattention mapã€‚

### Value

![Value](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_225834831.png)

valueçš„è®¡ç®—æ–¹å¼å’Œkeyç›¸åŒï¼Œä¹Ÿæ˜¯æ¯4ä¸ªvalueå¤´ä¹‹é—´å…±äº«æƒé‡ï¼Œå”¯ä¸€çš„åŒºåˆ«æ˜¯æ²¡æœ‰ä½ç½®ç¼–ç ã€‚

#### å±•å¼€Wv

```python
# åŠ è½½Wv
v_layer0 = model["layers.0.attention.wv.weight"]
# æœ‰8ä¸ªvalueå¤´çš„Wvï¼Œæ¯ä¸ªå¤´çš„Wkå½¢çŠ¶æ˜¯[128, 4096]
v_layer0 = v_layer0.view(n_kv_heads, v_layer0.shape[0] // n_kv_heads, dim) # [8, 128, 4096]

# ä»¥ç¬¬1ä¸ªå¤´ä¸ºä¾‹
v_layer0_head0 = k_layer0[0] # [128, 4096]
```

#### è®¡ç®—Value

![V](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_225915261.png)

```python
v_per_token = torch.matmul(token_embeddings, v_layer0_head0.T) # [17, 128]
```

æ¯ä¸ªtokençš„æ³¨æ„åŠ›valueï¼Œå½¢çŠ¶æ˜¯[17, 128]ï¼Œè¡¨ç¤ºæœ‰17ä¸ªtokenï¼Œæ¯ä¸ªtokençš„valueå‘é‡é•¿åº¦ä¸º128ã€‚

### è®¡ç®—Attention

ç»ˆäºå¯ä»¥å®Œæ•´çš„è®¡ç®—Attentionå…¬å¼äº†ã€‚

![Attention](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_230006680.png)

attention mapä¸­çš„0~1å€¼ï¼Œç›¸å½“äºæ˜¯ä¸€ä¸ªåŠ æƒå€¼ï¼Œç”¨æ¥ç¡®å®šæ¯ä¸ªtokenéœ€è¦ç”¨åˆ°å¤šå°‘valueã€‚

```python
qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token) # [17, 128]
```

## å¤šå¤´Attention

![Step of Multi-head self attention](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_230139442.png)

å¤šå¤´æ³¨æ„åŠ›çš„ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![Multi-head self attention](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_230552118.png)

### è®¡ç®—æ¯ä¸ªå¤´

å‰é¢æåˆ°ï¼Œè¿™ä¸ªæ³¨æ„åŠ›æ¨¡å—æœ‰32ä¸ªå¤´ï¼Œä¸Šé¢åªæ˜¯ä»¥ç¬¬1ä¸ªå¤´ä¸ºä¾‹ï¼Œç°åœ¨æˆ‘ä»¬è¦å†™ä¸ªå¾ªç¯ï¼ŒæŠŠæ‰€æœ‰å¤´éƒ½è®¡ç®—å‡ºæ¥ï¼Œè¿ç®—è¿‡ç¨‹å®Œå…¨ç›¸åŒã€‚

![head](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_230213701.png)

```python
qkv_attention_store = []

for head in range(n_heads):
    # åŠ è½½qkvçš„æŸä¸ªå¤´çš„æƒé‡
    q_layer0_head = q_layer0[head] # [128, 4096]
    k_layer0_head = k_layer0[head//4] # [128, 4096], key weights are shared across 4 heads
    v_layer0_head = v_layer0[head//4] # [128, 4096], value weights are shared across 4 heads
    
    # è®¡ç®—qkv
    q_per_token = torch.matmul(token_embeddings, q_layer0_head.T) # [17, 128]
    k_per_token = torch.matmul(token_embeddings, k_layer0_head.T) # [17, 128]
    v_per_token = torch.matmul(token_embeddings, v_layer0_head.T) # [17, 128]
	
    # è®¡ç®—qçš„ä½ç½®ç¼–ç 
    q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2) # [17, 64, 2]
    q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs) # [17, 64]
    q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers * freqs_cis[:len(tokens)]) # [17, 64, 2]
    q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape) # [17, 128]
	
    # è®¡ç®—kçš„ä½ç½®ç¼–ç 
    k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2) # [17, 64, 2]
    k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs) # [17, 64]
    k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis[:len(tokens)]) # [17, 64, 2]
    k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape) # [17, 128]
	
    # q*kT/sqrt(d)
    qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(128)**0.5 # [17, 17]
    
    # mask
    mask = torch.full((len(tokens), len(tokens)), float("-inf"), device=tokens.device) # [17, 17]
    mask = torch.triu(mask, diagonal=1) # [17, 17]
    qk_per_token_after_masking = qk_per_token + mask # [17, 17]
    
    # softmax
    qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16) # [17, 17]
    
    # attention
    qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token) # [17, 128]
    
    qkv_attention_store.append(qkv_attention)

len(qkv_attention_store) # 32
```

![Heads](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_230346818.png)

æˆ‘ä»¬ç°åœ¨æœ‰äº†ç¬¬1å±‚æ‰€æœ‰32ä¸ªå¤´çš„`qkv_attention`çŸ©é˜µï¼Œæ¥ä¸‹æ¥æˆ‘è¦æŠŠå®ƒä»¬åˆå¹¶æˆä¸€ä¸ªå¤§å°ä¸º 17 x 4096çš„å¤§çŸ©é˜µï¼š

```python
stacked_qkv_attention = torch.cat(qkv_attention_store, dim=-1) # [17, 4096]
```

### æ³¨æ„åŠ›èåˆ

ç»ˆæ­¢åˆ°äº†æ³¨æ„åŠ›æ¨¡å—çš„æœ€åä¸€æ­¥ï¼

![Concatenate](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_230738771.png)

å‰é¢å¾—åˆ°äº†32ä¸ªå¤´çš„Attentionå€¼ï¼Œè¿™é‡Œéœ€è¦åšä¸€ä¸ªçº¿æ€§å˜æ¢ï¼Œå°†ä»–ä»¬èåˆèµ·æ¥ï¼š

```python
# åŠ è½½Wo
w_layer0 = model["layers.0.attention.wo.weight"] # [4096, 4096]
# æ³¨æ„åŠ›èåˆ
embedding_delta = torch.matmul(stacked_qkv_attention, w_layer0.T) # [17, 4096]
```

### æ®‹å·®è¿æ¥

æˆ‘ä»¬å†æ·»åŠ ä¸€ä¸ªæ®‹å·®è¿æ¥ï¼Œå³ï¼šâ€œAttentionçš„è¾“å…¥å€¼ï¼ˆæœªnormï¼‰+ è¾“å‡ºå€¼"ã€‚

![Step of skip connection](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_230816308.png)

å¯ä»¥ä½¿å¾—æ¨¡å‹æ›´å®¹æ˜“åœ°å­¦ä¹ åˆ°æ’ç­‰æ˜ å°„ï¼Œä»è€Œé¿å…äº†è®­ç»ƒæ·±åº¦ç½‘ç»œæ—¶å¸¸è§çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼š

```python
embedding_after_edit = token_embeddings_unnormalized + embedding_delta # [17, 4096]
```

å¯¹ç»“æœå†æ¥ä¸€æ¬¡RMSå½’ä¸€åŒ–ï¼š

![Step of rms_norm](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_230839159.png)

```python
embedding_after_edit_normalized = rms_norm(embedding_after_edit, model["layers.0.ffn_norm.weight"]) # [17, 4096]
```

## FFN

### SwiGLU

![Step of FFN](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_230922540.png)

åœ¨Llama3ä¸­ï¼Œä½¿ç”¨äº†SwiGLUæ¿€æ´»å‡½æ•°çš„å‰é¦ˆç½‘ç»œï¼Œè¿™ç§ç½‘ç»œæ¶æ„éå¸¸é€‚åˆåœ¨æ¨¡å‹éœ€è¦æ—¶æ·»åŠ éçº¿æ€§ã€‚

SwiGLUåˆ°åº•æœ‰ä»€ä¹ˆä¼˜ç‚¹ï¼Œè¿™é‡Œå¼ºè¡Œè§£é‡Šä¸€æ³¢ï¼š

- Swishå¯¹äºè´Ÿå€¼çš„å“åº”ç›¸å¯¹è¾ƒå°ï¼Œå…‹æœäº† ReLU æŸäº›ç¥ç»å…ƒä¸Šè¾“å‡ºå§‹ç»ˆä¸ºé›¶çš„ç¼ºç‚¹
- GLU çš„é—¨æ§ç‰¹æ€§ï¼Œè¿™æ„å‘³ç€å®ƒå¯ä»¥æ ¹æ®è¾“å…¥çš„æƒ…å†µå†³å®šå“ªäº›ä¿¡æ¯åº”è¯¥é€šè¿‡ã€å“ªäº›ä¿¡æ¯åº”è¯¥è¢«è¿‡æ»¤ã€‚è¿™ç§æœºåˆ¶å¯ä»¥ä½¿ç½‘ç»œæ›´æœ‰æ•ˆåœ°å­¦ä¹ åˆ°æœ‰ç”¨çš„è¡¨ç¤ºï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­ï¼Œè¿™å¯¹äºå¤„ç†é•¿åºåˆ—ã€é•¿è·ç¦»ä¾èµ–çš„æ–‡æœ¬ç‰¹åˆ«æœ‰ç”¨
- SwiGLU ä¸­çš„å‚æ•°å¯ä»¥é€šè¿‡è®­ç»ƒå­¦ä¹ ï¼Œä½¿å¾—æ¨¡å‹å¯ä»¥æ ¹æ®ä¸åŒä»»åŠ¡å’Œæ•°æ®é›†åŠ¨æ€è°ƒæ•´è¿™äº›å‚æ•°ï¼Œå¢å¼ºäº†æ¨¡å‹çš„çµæ´»æ€§å’Œé€‚åº”æ€§
- è®¡ç®—æ•ˆç‡ç›¸æ¯”æŸäº›è¾ƒå¤æ‚çš„æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ GELUï¼‰æ›´é«˜ï¼ŒåŒæ—¶ä»èƒ½ä¿æŒè¾ƒå¥½çš„æ€§èƒ½ã€‚è¿™å¯¹äºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†æ˜¯å¾ˆé‡è¦çš„è€ƒé‡å› ç´ 

![SwiGLU](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_232619760.png)

```python
w1 = model["layers.0.feed_forward.w1.weight"] # [14336, 4096]
w2 = model["layers.0.feed_forward.w2.weight"] # [4096, 14336]
w3 = model["layers.0.feed_forward.w3.weight"] # [14336, 4096]

fc_up = torch.matmul(embedding_after_edit_normalized, w3.T) # [17, 14336]
fc_gate = torch.functional.F.silu(torch.matmul(embedding_after_edit_normalized, w1.T)) # [17, 14336]
output_after_feedforward = torch.matmul(fc_gate * fc_up, w2.T) # [17, 4096]
```

### æ®‹å·®è¿æ¥

æˆ‘ä»¬å†æ·»åŠ ä¸€ä¸ªæ®‹å·®è¿æ¥ï¼Œå³ï¼šâ€œFFNçš„è¾“å…¥å€¼ï¼ˆæœªnormï¼‰+ è¾“å‡ºå€¼"ã€‚

![Step of skip connection](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_232723695.png)

å¯ä»¥ä½¿å¾—æ¨¡å‹æ›´å®¹æ˜“åœ°å­¦ä¹ åˆ°æ’ç­‰æ˜ å°„ï¼Œä»è€Œé¿å…äº†è®­ç»ƒæ·±åº¦ç½‘ç»œæ—¶å¸¸è§çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼š

```python
layer_0_embedding = embedding_after_edit + output_after_feedforward  # [17, 4096]
```

# æ„å»ºæ‰€æœ‰Transformerå±‚

Llama3ä¸€å…±æœ‰32å±‚Transformerï¼Œæˆ‘åªéœ€è¦æŒ‰ç…§åŒæ ·çš„æ–¹å¼ï¼Œå†™ä¸ªforå¾ªç¯ï¼Œå®ç°å‰©ä¸‹çš„31å±‚å°±è¡Œï¼Œæ¯ä¸€å±‚çš„è¾“å…¥éƒ½æ˜¯ä¸Šä¸€å±‚çš„è¾“å‡ºã€‚

![All transformer layers](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_232936335.png)

```python
# embeddings input
final_embedding = token_embeddings_unnormalized

# each layer
for layer in range(n_layers):
    qkv_attention_store = []
    
    # rms_norm
    layer_embedding_norm = rms_norm(final_embedding, model[f"layers.{layer}.attention_norm.weight"]) # [17, 4096]
    
    # qkv weight for each layer
    q_layer = model[f"layers.{layer}.attention.wq.weight"] # [4096, 4096]
    q_layer = q_layer.view(n_heads, q_layer.shape[0] // n_heads, dim) # [32, 128, 4096]
    k_layer = model[f"layers.{layer}.attention.wk.weight"] # [1024, 4096]
    k_layer = k_layer.view(n_kv_heads, k_layer.shape[0] // n_kv_heads, dim) # [8, 128, 4096]
    v_layer = model[f"layers.{layer}.attention.wv.weight"] # [1024, 4096]
    v_layer = v_layer.view(n_kv_heads, v_layer.shape[0] // n_kv_heads, dim) # [8, 128, 4096]
    w_layer = model[f"layers.{layer}.attention.wo.weight"] # [4096, 4096]
    
    # each head
    for head in range(n_heads):
        # weight for head
        q_layer_head = q_layer[head] # [128, 4096]
        k_layer_head = k_layer[head//4] # [128, 4096]
        v_layer_head = v_layer[head//4] # [128, 4096]
        
        # qkv
        q_per_token = torch.matmul(layer_embedding_norm, q_layer_head.T) # [17, 128]
        k_per_token = torch.matmul(layer_embedding_norm, k_layer_head.T) # [17, 128]
        v_per_token = torch.matmul(layer_embedding_norm, v_layer_head.T) # [17, 128]
        
        # rope
        q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2) # [17, 64, 2]
        q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs) # [17, 64]
        q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers * freqs_cis) # [17, 64, 2]
        q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape) # [17, 128]
        
        k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2) # [17, 64, 2]
        k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs) # [17, 64]
        k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis) # [17, 64, 2]
        k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape) # [17, 128]
        
         # q*kT/sqrt(d)
        qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(q_per_token_rotated.shape[-1])**0.5 # [17, 17]
        
        # mask
        mask = torch.full((len(token_embeddings_unnormalized), len(token_embeddings_unnormalized)), float("-inf")) # [17, 17]
        mask = torch.triu(mask, diagonal=1) # [17, 17]
        qk_per_token_after_masking = qk_per_token + mask # [17, 17]
        
        # softmax
        qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16) # [17, 17]
        
        # attention
        qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token) # [17, 128]
        qkv_attention_store.append(qkv_attention)
	
    # stack
    stacked_qkv_attention = torch.cat(qkv_attention_store, dim=-1) # [17, 4096]
    
    # attention
    w_layer = model[f"layers.{layer}.attention.wo.weight"] # [4096, 4096]
    embedding_delta = torch.matmul(stacked_qkv_attention, w_layer.T) # [17, 4096]
    
    # skip-connection
    embedding_after_edit = final_embedding + embedding_delta # [17, 4096]
    
    # rms norm
    embedding_after_edit_normalized = rms_norm(embedding_after_edit, model[f"layers.{layer}.ffn_norm.weight"]) # [17, 4096]
    
    # ffn
    w1 = model["layers.0.feed_forward.w1.weight"] # [14336, 4096]
    w2 = model["layers.0.feed_forward.w2.weight"] # [4096, 14336]
    w3 = model["layers.0.feed_forward.w3.weight"] # [14336, 4096]
    fc_up = torch.matmul(embedding_after_edit_normalized, w3.T) # [17, 14336]
    fc_gate = torch.functional.F.silu(torch.matmul(embedding_after_edit_normalized, w1.T)) # [17, 14336]
    output_after_feedforward = torch.matmul(fc_gate * fc_up, w2.T) # [17, 4096]
    
    # skip-connection
    final_embedding = embedding_after_edit+output_after_feedforward
```

ç„¶åï¼Œæˆ‘ä»¬å¯¹è¾“å‡ºç»“æœå†åšä¸€æ¬¡RMSå½’ä¸€åŒ–ï¼š

```python
final_embedding = rms_norm(final_embedding, model["norm.weight"]) # [17, 4096]
```

# æ„å»ºTransformerçš„è¾“å‡º

![Step of output](https://raw.githubusercontent.com/liangxhao/blogs/markdown/imgs/2024/06/20240622_233044019.png)

å‡è®¾æˆ‘ä»¬ç«™åœ¨æœ€å1ä¸ªtokenï¼Œå³ç¬¬17ä¸ªtokençš„ä¸Šï¼Œé¢„æµ‹ç¬¬18ä¸ªtokençš„æ¦‚ç‡åˆ†å¸ƒï¼Œè¿™ä¸ªæ¦‚ç‡åˆ†å¸ƒè‚¯å®šæ˜¯ä¸€ä¸ªé•¿åº¦ä¸ºvocab_sizeçš„å‘é‡ï¼Œè¡¨ç¤ºæ¯ä¸ªè¯è¢«å‘½ä¸­çš„æ¦‚ç‡ã€‚

é‚£ä¹ˆï¼Œåªéœ€è¦åšä¸€ä¸ªæ˜ å°„å³å¯ï¼š

```python
logits = torch.matmul(final_embedding[-1], model["output.weight"].T) # [128256]
```

æ­¤æ—¶ï¼Œ`logits`æ˜¯æœªå½’ä¸€åŒ–0~1ä¹‹é—´çš„ï¼Œå¦‚æœè¦å¾—åˆ°æ¦‚ç‡å€¼ï¼ŒåŠ ä¸€æ­¥`softmax`å³å¯ã€‚

åœ¨æ¨ç†æ—¶ï¼Œæˆ‘ä»¬åªæƒ³å¾—åˆ°æ¦‚ç‡æœ€å¤§å€¼å¯¹åº”çš„tokenï¼Œé‚£ç›´æ¥å–`argmax`å³å¯ï¼š

```python
next_token = torch.argmax(logits, dim=-1)
print(next_token)
# tensor(2983)
```

æœ‰äº†tokenä¹‹åï¼Œè°ƒç”¨åˆ†è¯å™¨è§£ç ï¼Œå°±å¯ä»¥å¾—åˆ°æ–‡æœ¬è¾“å‡ºï¼š

```python
output = tokenizer.decode([next_token.item()]) 
print(output)
# '42'
```

# ç»“è¯­

è¿™åªæ˜¯ä¸€ä¸ªååˆ†ç®€ç•¥çš„å®ç°ï¼ŒæŒ‰ç…§è¿™äº›æ­¥éª¤ä¸€æ­¥æ­¥å®ç°ï¼Œè¯¦ç»†å¤§å®¶å¯ä»¥å¯¹Llama3ä»¥åŠå¸¸è§çš„LLMçš„æ¨¡å‹ç»“æ„ï¼Œæœ‰ä¸ªç›´è§‚çš„äº†è§£ã€‚

å®é™…çš„è®­ç»ƒå’Œæ¨ç†ï¼Œè‚¯å®šè¦ç»è¿‡å„ç§æ€§èƒ½ä¼˜åŒ–ï¼Œéœ€è¦å¯¹ä»£ç åšè¾ƒå¤§æ”¹åŠ¨ã€‚